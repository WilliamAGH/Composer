# Server Configuration
server.port=${PORT:8090}
server.address=0.0.0.0
# Extend servlet session lifetime to avoid frequent nonce expirations (configurable via env).
server.servlet.session.timeout=${SESSION_TIMEOUT:PT8H}
# server.servlet.context-path=/api

# Application Configuration
spring.application.name=composerai-api

# Optional dotenv-style imports (loaded if present). Environment variables still take precedence.
spring.config.import=optional:file:.env,optional:file:.env.local,optional:file:.env.properties,optional:file:.env.local.properties

# ==========================================
# OpenAI Configuration
# Source of Truth: Defaults in OpenAiProperties.java
# Override via environment variables as needed
# 
# Alternative Providers:
# This application supports OpenAI-compatible providers (OpenRouter, Groq, LM Studio, etc.)
# Configure credentials via OPENAI_* variables; provider detection is automatic based on base URL.
# ==========================================

# API Credentials (required)
openai.api.key=${OPENAI_API_KEY:}
# Base URL for API endpoint
# Examples:
#   OpenAI: https://api.openai.com/v1 (default)
#   OpenRouter: https://openrouter.ai/api/v1
#   Groq: https://api.groq.com/openai/v1
#   LM Studio: http://localhost:1234/v1
openai.api.base-url=${OPENAI_BASE_URL:https://api.openai.com/v1}

# Chat Model (default: gpt-4o-mini)
# Driven entirely by LLM_MODEL for consistent infra usage
openai.model.chat=${LLM_MODEL:gpt-4o-mini}

# Model Settings (temperature, token limits, sampling)
openai.model.temperature=${LLM_TEMPERATURE:0.5}
openai.model.max-output-tokens=${LLM_MAX_OUTPUT_TOKENS:}
openai.model.top-p=${LLM_TOP_P:}

# Embedding Model (default: text-embedding-3-small)
openai.embedding.model=${OPENAI_EMBEDDING_MODEL:text-embedding-3-small}

# Streaming Configuration (defaults: 120s timeout, 10s heartbeat)
openai.stream.timeout-seconds=${OPENAI_STREAM_TIMEOUT:120}
openai.stream.heartbeat-interval-seconds=${OPENAI_STREAM_HEARTBEAT:10}

# Reasoning Configuration (defaults: o1,o3,o4,gpt-5 / low)
openai.reasoning.supported-model-prefixes=${OPENAI_REASONING_MODELS:o1,o3,o4,gpt-5}
openai.reasoning.default-effort=${LLM_REASONING:${OPENAI_REASONING_EFFORT:low}}

# Provider Routing (OpenRouter only)
# Controls which providers to use when base URL is https://openrouter.ai/api/v1
# See: https://openrouter.ai/docs/features/provider-routing
openai.provider.sort=${LLM_PROVIDER_SORT:}
openai.provider.order=${LLM_PROVIDER_ORDER:novita}
openai.provider.allow-fallbacks=${LLM_PROVIDER_ALLOW_FALLBACKS:true}

# Debug Configuration
openai.local-debug-enabled=${OPENAI_LOCAL_DEBUG:false}

# Intent Analysis (defaults: question / 10 tokens / standard categories)
openai.intent.default-category=${OPENAI_INTENT_DEFAULT:question}
openai.intent.max-output-tokens=${OPENAI_INTENT_MAX_TOKENS:10}
openai.intent.categories=${OPENAI_INTENT_CATEGORIES:search, compose, summarize, analyze, question, other}

# Request Defaults (defaults: 5 results / 4000 chars / thinking off)
openai.defaults.max-search-results=${OPENAI_MAX_SEARCH_RESULTS:5}
openai.defaults.max-message-length=${OPENAI_MAX_MESSAGE_LENGTH:4000}
openai.defaults.thinking-enabled=${OPENAI_THINKING_ENABLED:false}

# System Prompts (defaults in OpenAiProperties.java)
# Override only if custom prompts are needed; otherwise defaults from OpenAiProperties.java are used
# openai.prompts.email-assistant-system=${OPENAI_PROMPT_EMAIL}
# openai.prompts.intent-analysis-system=${OPENAI_PROMPT_INTENT}

# ==========================================
# Error Messages (defaults in ErrorMessagesProperties.java)
# ==========================================
# messages.openai.misconfigured=${MESSAGES_OPENAI_MISCONFIGURED}
# messages.openai.unavailable=${MESSAGES_OPENAI_UNAVAILABLE}
# messages.chat.processing-error=${MESSAGES_CHAT_ERROR}
# messages.stream.error=${MESSAGES_STREAM_ERROR}
# messages.stream.timeout=${MESSAGES_STREAM_TIMEOUT}

# Qdrant Configuration
qdrant.enabled=${QDRANT_ENABLED:false}
qdrant.host=${QDRANT_HOST:localhost}
qdrant.port=${QDRANT_PORT:6333}
qdrant.use-tls=${QDRANT_USE_TLS:false}
qdrant.collection-name=${QDRANT_COLLECTION_NAME:emails}
qdrant.api-key=${QDRANT_API_KEY:}

# Logging Configuration
logging.level.com.composerai.api=DEBUG
logging.level.com.composerai.api.config.ClientConfiguration=DEBUG
logging.level.root=INFO

# CORS
# Default to common origins; override via APP_CORS_ALLOWED_ORIGINS environment variable
app.cors.allowed-origins=${APP_CORS_ALLOWED_ORIGINS:http://localhost:8090,http://localhost:5183,https://composerai.app,https://dev.composerai.app}

# UI Feature Flags
# Dark mode for sidebar - disabled by default (light mode)
app.ui.dark-sidebar-enabled=${APP_DARK_SIDEBAR_ENABLED:false}
